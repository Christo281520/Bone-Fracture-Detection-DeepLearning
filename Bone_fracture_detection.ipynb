{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ux0MVqPiAdz"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"vuppalaadithyasairam/bone-fracture-detection-using-xrays\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLib8SCVkdcN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# used to find all the pathnames matching\n",
        "import glob as gb\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import classification_report , confusion_matrix , ConfusionMatrixDisplay,accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization , MaxPooling2D,Conv2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clhGz70zP_My"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/archive.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDq3Qrj5OB0S"
      },
      "outputs": [],
      "source": [
        "# model = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
        "#     MaxPooling2D(2,2),\n",
        "#     BatchNormalization(),\n",
        "\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2,2),\n",
        "#     BatchNormalization(),\n",
        "\n",
        "#     Conv2D(128, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2,2),\n",
        "#     BatchNormalization(),\n",
        "\n",
        "#     Flatten(),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(1, activation='sigmoid')  # Binary classification\n",
        "# ])\n",
        "\n",
        "\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# callbacks = [\n",
        "#     EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, restore_best_weights=True)\n",
        "# ]\n",
        "\n",
        "# history = model.fit(\n",
        "#     train_generator  ,\n",
        "#     validation_data=(test_generator),\n",
        "#     epochs=10,\n",
        "#     callbacks=callbacks\n",
        "# )\n",
        "\n",
        "# model.save(\"fracture.h5\")\n",
        "# print(\"Model saved as fracture.h5\")\n",
        "\n",
        "# with open(\"fracture.pkl\", \"wb\") as pkl_file:\n",
        "#     pickle.dump(model, pkl_file)\n",
        "# print(\"Model saved as fracture.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15-KxXUGyI8e"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "print(\"Archive extracted to /content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHNoIteqHU_u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check what's in /content\n",
        "print(\"Top-level files/folders after extraction:\")\n",
        "print(os.listdir(\"/content\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8EO5diXHnz7"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/archive (6)\"  # or whatever folder name appears\n",
        "print(os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPAI_QaPz-nr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# used to find all the pathnames matching\n",
        "import glob as gb\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import classification_report , confusion_matrix , ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization , MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSOuzkY_yc4v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path_train = r\"/content/archive (6)/train\"\n",
        "\n",
        "for folder in os.listdir(path=path_train) :\n",
        "    file_folder = os.path.join(path_train , folder)\n",
        "    files = gb.glob(pathname=os.path.join(file_folder , \"*.jpg\"))\n",
        "    print(f\"Training Data,Found {len(files)} in folder {folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTdLLhUt0Fo3"
      },
      "outputs": [],
      "source": [
        "path_test = r\"/content/archive (6)/val\"\n",
        "\n",
        "for folder in os.listdir(path=path_test) :\n",
        "    file_folder = os.path.join(path_test , folder)\n",
        "    files = gb.glob(pathname=os.path.join(file_folder , \"*.jpg\"))\n",
        "    print(f\"Testing Data,Found {len(files)} in folder {folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rftnZLW0Mya"
      },
      "outputs": [],
      "source": [
        "def segment_image(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (5, 5), 0) # standard deviation in the X and Y directions\n",
        "    _, thr = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY)\n",
        "                                        #  retrieval mode only retrieves the external contours of objects\n",
        "    contours, _ = cv2.findContours(thr, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # reduces the amount of data stored in the contour\n",
        "    mask = np.zeros_like(image)\n",
        "    cv2.drawContours(mask, contours, -1, (255, 255, 255), thickness=cv2.FILLED)\n",
        "    return cv2.bitwise_and(image, mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKwwmkVA0QhE"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image, method):\n",
        "    # Convert to grayscale for methods that require it\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # LBP also applies histogram equalization for better lighting normalization\n",
        "    if method == 'LBP':\n",
        "        gray = cv2.equalizeHist(gray)\n",
        "\n",
        "    return gray\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v4XUUy50ToL"
      },
      "outputs": [],
      "source": [
        "def feature_extraction_chain_code(image):\n",
        "    \"\"\"\n",
        "    Extract chain code features from the image.\n",
        "    \"\"\"\n",
        "    gray = prepare_image(image, 'chain_code')\n",
        "    ret, thresh = cv2.threshold(gray, 127, 255, 0)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    chain_code = []\n",
        "\n",
        "    for contour in contours:\n",
        "        contour = contour.reshape(-1, 2)\n",
        "        for i in range(1, len(contour)):\n",
        "            dx = contour[i][0] - contour[i-1][0]\n",
        "            dy = contour[i][1] - contour[i-1][1]\n",
        "            direction = (np.arctan2(dy, dx) + np.pi) // (np.pi / 4)\n",
        "            chain_code.append(int(direction))\n",
        "\n",
        "    # Ensure chain code is always 2048-length\n",
        "    return np.pad(chain_code, (0, 2048 - len(chain_code)), 'constant')[:2048]\n",
        "\n",
        "def regional_Descriptor(image):\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    inverted_binary = ~binary\n",
        "\n",
        "    _, thr = cv2.threshold(image, 127, 255, cv2.THRESH_OTSU)\n",
        "\n",
        "    inverted_binary = ~thr\n",
        "    contours, _ = cv2.findContours(inverted_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    with_contours=cv2.drawContours(image, contours, -1, (255, 255, 255))\n",
        "\n",
        "    object=contours[0]\n",
        "    print('contours= ',object)\n",
        "    area=cv2.contourArea(object)\n",
        "    perimeter=cv2.arcLength(object,True)\n",
        "    compactness=(4*np.pi*area)/(perimeter**2)\n",
        "\n",
        "    return area, perimeter, compactness\n",
        "\n",
        "def feature_extraction_glcm(image):\n",
        "    \"\"\"\n",
        "    Extract GLCM texture features from the image.\n",
        "    \"\"\"\n",
        "    gray = prepare_image(image, 'GLCM')\n",
        "    glcm = texture.greycomatrix(gray, [1], [0], symmetric=True, normed=True)\n",
        "\n",
        "    contrast = texture.greycoprops(glcm, 'contrast')[0, 0]\n",
        "    dissimilarity = texture.greycoprops(glcm, 'dissimilarity')[0, 0]\n",
        "    homogeneity = texture.greycoprops(glcm, 'homogeneity')[0, 0]\n",
        "\n",
        "    return np.array([contrast, dissimilarity, homogeneity])\n",
        "\n",
        "def feature_extraction_lbp(image):\n",
        "    \"\"\"\n",
        "    Extract LBP features from the image.\n",
        "    \"\"\"\n",
        "    gray = prepare_image(image, 'LBP')\n",
        "    radius = 1\n",
        "    n_points = 8 * radius\n",
        "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
        "    lbp_histogram, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 59), range=(0, 58))\n",
        "    lbp_histogram = lbp_histogram.astype('float')\n",
        "    lbp_histogram /= lbp_histogram.sum()  # Normalize to sum to 1\n",
        "\n",
        "    # Return first 2048 values or pad\n",
        "    return lbp_histogram[:2048]\n",
        "\n",
        "def feature_extraction(image, method='SIFT'):\n",
        "    \"\"\"\n",
        "    Call the appropriate feature extraction method.\n",
        "    \"\"\"\n",
        "    if method == 'chain_code':\n",
        "        return feature_extraction_chain_code(image)\n",
        "    elif method == 'regional':\n",
        "        return regional_Descriptor(image)\n",
        "    elif method == 'GLCM':\n",
        "        return feature_extraction_glcm(image)\n",
        "    elif method == 'LBP':\n",
        "        return feature_extraction_lbp(image)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMGMHDvE0inT"
      },
      "outputs": [],
      "source": [
        "labels = {\"fractured\" : 0 , \"not fractured\" : 1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9NFUpfD0lMP"
      },
      "outputs": [],
      "source": [
        "def load_data(path, target_size=(224, 224)):\n",
        "    X, y = [], []\n",
        "    labels = {'fractured': 0, 'not fractured': 1}  # Update based on folder names\n",
        "    for folder in os.listdir(path):\n",
        "        folder_path = os.path.join(path, folder)\n",
        "        files = gb.glob(os.path.join(folder_path, \"*.jpg\"))\n",
        "        for file in files:\n",
        "            image = cv2.imread(file)\n",
        "            if image is not None:  # Check if image is loaded correctly\n",
        "                image = cv2.resize(image, target_size)  # Resize image\n",
        "                X.append(image)\n",
        "                y.append(labels[folder])\n",
        "    return np.array(X), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJfhtZMa0n_a"
      },
      "outputs": [],
      "source": [
        " #Load data\n",
        "X_train, y_train = load_data(path_train)\n",
        "X_test, y_test = load_data(path_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxJvN0ii0shJ"
      },
      "outputs": [],
      "source": [
        "print(f\"Len X Train Is == {len(X_train)}\")\n",
        "print(f\"Len X Test Is == {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfkObHXh06Rm"
      },
      "outputs": [],
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X_train , y_train , test_size=0.2 , random_state=42 , shuffle=True)\n",
        "\n",
        "\n",
        "print(f\"X Train Shape is ==> {X_train.shape}\")\n",
        "print(f\"X test Shape is ==> {X_test.shape}\")\n",
        "print(f\"y train Shape is ==> {y_train.shape}\")\n",
        "print(f\"y test Shape is ==> {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al0vPa141Fgc"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "\n",
        "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
        "test_generator = datagen.flow(X_test, y_test, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeNdqnQR1by6",
        "outputId": "78ddce94-5ebc-401d-cc4f-19761444e29e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 2s/step - accuracy: 0.5483 - loss: 0.9156 - val_accuracy: 0.6345 - val_loss: 0.6483\n",
            "Epoch 2/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 2s/step - accuracy: 0.6167 - loss: 0.6912 - val_accuracy: 0.6853 - val_loss: 0.5833\n",
            "Epoch 3/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 2s/step - accuracy: 0.6678 - loss: 0.6214 - val_accuracy: 0.7236 - val_loss: 0.5321\n",
            "Epoch 4/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 2s/step - accuracy: 0.7095 - loss: 0.5581 - val_accuracy: 0.7879 - val_loss: 0.4510\n",
            "Epoch 5/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 2s/step - accuracy: 0.7673 - loss: 0.4938 - val_accuracy: 0.7360 - val_loss: 0.5638\n",
            "Epoch 6/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 1s/step - accuracy: 0.7723 - loss: 0.4701 - val_accuracy: 0.7783 - val_loss: 0.4605\n",
            "Epoch 7/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 2s/step - accuracy: 0.7900 - loss: 0.4537 - val_accuracy: 0.8483 - val_loss: 0.3585\n",
            "Epoch 8/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.8098 - loss: 0.4159 - val_accuracy: 0.8331 - val_loss: 0.3735\n",
            "Epoch 9/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 2s/step - accuracy: 0.8105 - loss: 0.4105 - val_accuracy: 0.8691 - val_loss: 0.3095\n",
            "Epoch 10/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 1s/step - accuracy: 0.8185 - loss: 0.3976 - val_accuracy: 0.8466 - val_loss: 0.3615\n",
            "Epoch 11/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.8236 - loss: 0.3969 - val_accuracy: 0.8218 - val_loss: 0.3954\n",
            "Epoch 12/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 1s/step - accuracy: 0.8158 - loss: 0.3997 - val_accuracy: 0.7045 - val_loss: 0.5869\n",
            "Epoch 13/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.8390 - loss: 0.3705 - val_accuracy: 0.8144 - val_loss: 0.3818\n",
            "Epoch 14/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 1s/step - accuracy: 0.8356 - loss: 0.3699 - val_accuracy: 0.8872 - val_loss: 0.2761\n",
            "Epoch 15/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 2s/step - accuracy: 0.8364 - loss: 0.3672 - val_accuracy: 0.8601 - val_loss: 0.3283\n",
            "Epoch 16/30\n",
            "\u001b[1m203/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.8321 - loss: 0.3797"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam # Import the Adam optimizer\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(224, 224, 3)),  # Flattening the 2D image into 1D\n",
        "\n",
        "    Dense(512, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy']) # Now Adam is defined\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=test_generator,\n",
        "    epochs=30,\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"fracture_ann.h5\")\n",
        "print(\"Model saved as fracture_ann.h5\")\n",
        "\n",
        "with open(\"fracture_ann.pkl\", \"wb\") as pkl_file:\n",
        "    pickle.dump(model, pkl_file)\n",
        "print(\"Model saved as fracture_ann.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VJ8nW4D1kvk"
      },
      "outputs": [],
      "source": [
        "loss , acc = model.evaluate(train_generator)\n",
        "print(f\"Loss Training is == > {loss}\")\n",
        "print(f\"Accurcy Training is == > {acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaT_ogUW2Mix"
      },
      "outputs": [],
      "source": [
        "loss , acc = model.evaluate(test_generator)\n",
        "print(f\"Loss Testing is == > {loss}\")\n",
        "\n",
        "print(f\"Accurcy Testing is == > {acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF1K67cs2RgH"
      },
      "outputs": [],
      "source": [
        "def getlab(n):\n",
        "    labels = {\"fractured\" : 0 , \"not fractured\" : 1} # Define labels within the function scope\n",
        "    for x, y in labels.items():\n",
        "        if n == y:\n",
        "            return x\n",
        "\n",
        "plt.figure(figsize=(40, 20))\n",
        "num_images = 6\n",
        "\n",
        "sample_indices = np.random.randint(0, len(X_train), num_images)\n",
        "\n",
        "for n, i in enumerate(sample_indices):\n",
        "    # Read the original image\n",
        "    img_path = gb.glob(os.path.join(path_train, \"*\", \"*.jpg\"))[i]\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Column 1: Original image\n",
        "    plt.subplot(num_images, 3, n * 3 + 1)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Original: {getlab(y_train[i])}\", color=\"blue\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Column 2: Segmented image\n",
        "    segmented_img = segment_image(img)\n",
        "    plt.subplot(num_images, 3, n * 3 + 2)\n",
        "    plt.imshow(cv2.cvtColor(segmented_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Segmented\", color=\"green\", fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Column 3: Image with feature extraction\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints = sift.detect(segmented_img, None)\n",
        "    img_with_keypoints = cv2.drawKeypoints(segmented_img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "    plt.subplot(num_images, 3, n * 3 + 3)\n",
        "    plt.imshow(cv2.cvtColor(img_with_keypoints, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Features\", color=\"red\", fontsize=20)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9oZe4iILXRF"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(test_generator)\n",
        "pred = (pred > 0.5).astype(int)\n",
        "print(pd.DataFrame(pred[:10] , columns=[\"Prediction\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZCJXJF8Ldk-"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_pred=pred , y_true=y_test)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMEAbU0iLkmG"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fractured\", \"Not Fractured\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRRBwRUcL7Tf"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pred, target_names=[\"fractured\", \"not fractured\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BU-IE7FMHM7"
      },
      "outputs": [],
      "source": [
        "len(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyzd2UchNEJA"
      },
      "outputs": [],
      "source": [
        "num_images = 10\n",
        "\n",
        "# Randomly select indices             # 1773        10\n",
        "sample_indices = np.random.randint(0, len(X_test), num_images)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for n, i in enumerate(sample_indices):\n",
        "    actual_img = X_test[i]\n",
        "    actual_label = y_test[i]\n",
        "\n",
        "    pred_label = (pred[i] > 0.5).astype(int)\n",
        "\n",
        "    plt.subplot(num_images, 2, n * 2 + 1)\n",
        "    plt.imshow(cv2.cvtColor(actual_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Actual: {getlab(actual_label)}\", color=\"blue\", fontsize=12)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(num_images, 2, n * 2 + 2)\n",
        "    plt.imshow(cv2.cvtColor(actual_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Predicted: {getlab(pred_label)}\", color=\"red\", fontsize=12)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycogFaxX0qyH"
      },
      "outputs": [],
      "source": [
        "# prompt: bro make a ui to predict\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load your trained model\n",
        "model = load_model(\"fracture_ann.h5\")\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  # Normalize the image\n",
        "    return img_array\n",
        "\n",
        "# Function to predict the class\n",
        "def predict_fracture(img_path):\n",
        "    processed_image = preprocess_image(img_path)\n",
        "    prediction = model.predict(processed_image)\n",
        "    if prediction[0][0] > 0.5:\n",
        "        return \"Not Fractured\"\n",
        "    else:\n",
        "        return \"Fractured\"\n",
        "\n",
        "\n",
        "# UI elements\n",
        "upload_button = widgets.FileUpload(accept='image/*', multiple=False)\n",
        "output_label = widgets.Label()\n",
        "\n",
        "def on_upload_change(change):\n",
        "    if upload_button.value:\n",
        "        uploaded_file = list(upload_button.value.values())[0]\n",
        "        file_name = list(upload_button.value.keys())[0]\n",
        "\n",
        "        with open(file_name, 'wb') as f:\n",
        "          f.write(uploaded_file['content'])\n",
        "\n",
        "        prediction_result = predict_fracture(file_name)\n",
        "        output_label.value = f\"Prediction: {prediction_result}\"\n",
        "        clear_output(wait=True)\n",
        "        display(upload_button, output_label)\n",
        "\n",
        "upload_button.observe(on_upload_change, names='_counter')\n",
        "\n",
        "# Display the UI\n",
        "display(upload_button, output_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WqCYGBv8Ft1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "!pip install gradio --upgrade\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Assuming your model is loaded and ready to use as 'model'\n",
        "\n",
        "def predict_image(image):\n",
        "  # Preprocess the image (resize, normalize, etc.) as needed for your model\n",
        "  image = cv2.resize(image, (224, 224)) # Resize the image to (224, 224) here\n",
        "  image = image / 255.0\n",
        "  image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "\n",
        "  prediction = model.predict(image)\n",
        "  predicted_label = (prediction > 0.5).astype(int)[0][0]\n",
        "\n",
        "  return \"Fractured\" if predicted_label == 1 else \"Not Fractured\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=gr.Image(),  # Remove the 'shape' argument\n",
        "    outputs=\"text\",\n",
        "    title=\"Bone Fracture Detection\",\n",
        "    description=\"Upload an X-ray image to predict if it shows a fracture.\",\n",
        "\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bea06T0iGDp"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# # Load the trained model\n",
        "# model = tf.keras.models.load_model(\"fracture.h5\")\n",
        "\n",
        "# # Print model summary in text format\n",
        "# model.summary()\n",
        "\n",
        "# # Save and display model architecture as an image\n",
        "# plot_model(model, to_file=\"model_architecture.png\", show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# # If using Jupyter Notebook, display the image\n",
        "# from IPython.display import Image\n",
        "# Image(filename=\"model_architecture.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DijzWxRaf47w"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model # Import the plot_model function\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Define ANN model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(224, 224, 3)),  # Flattening the 2D image into 1D\n",
        "\n",
        "    Dense(512, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Visualize model architecture\n",
        "plot_model(model, to_file=\"model_architecture.png\", show_shapes=True, show_layer_names=True) # Now the model has layers defined"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}